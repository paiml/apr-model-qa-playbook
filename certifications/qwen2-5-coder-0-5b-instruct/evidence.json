[
  {
    "id": "0000000000000000189015e03a73163c",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 21465.83ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 12915.9ms (2.5 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.0,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 22195
    },
    "timestamp": "2026-02-01T09:38:26.354086933Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189015e95e90d069",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000001",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 1,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 9\nlatency: 39245.29ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "\n[PMAT-172] ERROR: APR file missing embedded tokenizer.\n           APR format requires self-contained tokenizer.\n           Re-convert with: apr convert <source>.gguf -o /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n           Or use the original GGUF file directly.\n\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[GH-189] Loaded tokenizer from /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-0.5B-Instruct/snapshots/ea3f2471cf1b1f0db85067f1ef93848e38e88c25/tokenizer.json: 22 special tokens\nGenerated 32 tokens in 34492.8ms (0.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 39260
    },
    "timestamp": "2026-02-01T09:39:05.614720589Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189015e9a4b88aaa",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_safetensors_0000000000000002",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 2,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): \n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1176
    },
    "timestamp": "2026-02-01T09:39:06.791730535Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189015ef4fec599a",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000003",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 3,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 23409.68ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 13050.7ms (2.5 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.9,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 24347
    },
    "timestamp": "2026-02-01T09:39:31.138865242Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189015f8a61efc06",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_apr_0000000000000004",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 4,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 9\nlatency: 40086.04ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "\n[PMAT-172] ERROR: APR file missing embedded tokenizer.\n           APR format requires self-contained tokenizer.\n           Re-convert with: apr convert <source>.gguf -o /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n           Or use the original GGUF file directly.\n\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[GH-189] Loaded tokenizer from /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-0.5B-Instruct/snapshots/ea3f2471cf1b1f0db85067f1ef93848e38e88c25/tokenizer.json: 22 special tokens\nGenerated 32 tokens in 35151.3ms (0.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 40100
    },
    "timestamp": "2026-02-01T09:40:11.239730503Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189015f9047582f9",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_safetensors_0000000000000005",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 5,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): \n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1582
    },
    "timestamp": "2026-02-01T09:40:12.822460181Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189015fe5d7358a4",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_cpu_gguf_0000000000000006",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 6,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 22129.44ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 12947.7ms (2.5 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.0,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 22967
    },
    "timestamp": "2026-02-01T09:40:35.790325714Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890160782c826ae",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_cpu_apr_0000000000000007",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 7,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 9\nlatency: 39266.13ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "\n[PMAT-172] ERROR: APR file missing embedded tokenizer.\n           APR format requires self-contained tokenizer.\n           Re-convert with: apr convert <source>.gguf -o /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n           Or use the original GGUF file directly.\n\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[GH-189] Loaded tokenizer from /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-0.5B-Instruct/snapshots/ea3f2471cf1b1f0db85067f1ef93848e38e88c25/tokenizer.json: 22 special tokens\nGenerated 32 tokens in 34413.4ms (0.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 39280
    },
    "timestamp": "2026-02-01T09:41:15.071346090Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901607caf3710a",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_cpu_safetensors_0000000000000008",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 8,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): \n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1210
    },
    "timestamp": "2026-02-01T09:41:16.282143757Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890160d5245c6e4",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_gpu_gguf_0000000000000009",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 9,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 22896.92ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 12932.6ms (2.5 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.0,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 23745
    },
    "timestamp": "2026-02-01T09:41:40.027298975Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901616d9a4245c",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_gpu_apr_000000000000000a",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 10,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 9\nlatency: 40910.09ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "\n[PMAT-172] ERROR: APR file missing embedded tokenizer.\n           APR format requires self-contained tokenizer.\n           Re-convert with: apr convert <source>.gguf -o /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n           Or use the original GGUF file directly.\n\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[GH-189] Loaded tokenizer from /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-0.5B-Instruct/snapshots/ea3f2471cf1b1f0db85067f1ef93848e38e88c25/tokenizer.json: 22 special tokens\nGenerated 32 tokens in 35995.0ms (0.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 40925
    },
    "timestamp": "2026-02-01T09:42:20.953113212Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890161721631a8f",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_gpu_safetensors_000000000000000b",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 11,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): \n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1203
    },
    "timestamp": "2026-02-01T09:42:22.156811176Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890161c58439d2e",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_cpu_gguf_000000000000000c",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 12,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 21512.01ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 12800.2ms (2.5 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.0,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 22395
    },
    "timestamp": "2026-02-01T09:42:44.552329961Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901625b7295685",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_cpu_apr_000000000000000d",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 13,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 9\nlatency: 40233.31ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "\n[PMAT-172] ERROR: APR file missing embedded tokenizer.\n           APR format requires self-contained tokenizer.\n           Re-convert with: apr convert <source>.gguf -o /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n           Or use the original GGUF file directly.\n\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[GH-189] Loaded tokenizer from /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-0.5B-Instruct/snapshots/ea3f2471cf1b1f0db85067f1ef93848e38e88c25/tokenizer.json: 22 special tokens\nGenerated 32 tokens in 35379.8ms (0.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 40246
    },
    "timestamp": "2026-02-01T09:43:24.799149771Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901625ff458838",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_cpu_safetensors_000000000000000e",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 14,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): \n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1209
    },
    "timestamp": "2026-02-01T09:43:26.008957124Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890162b681eb62e",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_gpu_gguf_000000000000000f",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 15,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 22361.36ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 12813.9ms (2.5 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.0,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 23233
    },
    "timestamp": "2026-02-01T09:43:49.242856535Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901634b7fed6d4",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_gpu_apr_0000000000000010",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 16,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 9\nlatency: 39979.11ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "\n[PMAT-172] ERROR: APR file missing embedded tokenizer.\n           APR format requires self-contained tokenizer.\n           Re-convert with: apr convert <source>.gguf -o /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n           Or use the original GGUF file directly.\n\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[GH-189] Loaded tokenizer from /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-0.5B-Instruct/snapshots/ea3f2471cf1b1f0db85067f1ef93848e38e88c25/tokenizer.json: 22 special tokens\nGenerated 32 tokens in 35147.1ms (0.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 39994
    },
    "timestamp": "2026-02-01T09:44:29.237650184Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901634ff2f7439",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_gpu_safetensors_0000000000000011",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 17,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): \n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n\n[PMAT-172] ERROR: No tokenizer found for /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors.\n           Expected sibling file: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/tokenizer.json\n           For SafeTensors models, tokenizer.json must be in same directory.\n\nerror: Inference failed: Inference failed: Operation 'safetensors_convert' not supported: config.json not found (required for SafeTensors inference)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1194
    },
    "timestamp": "2026-02-01T09:44:30.432019893Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901634ff624715",
    "gate_id": "F-CONV-G-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Convert Gguf to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.435349723Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901634ff90181e",
    "gate_id": "F-CONV-G-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Convert Gguf to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.438352435Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901634ffc1f722",
    "gate_id": "F-CONV-A-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Convert Apr to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.441618518Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901634ffefbce6",
    "gate_id": "F-CONV-A-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Convert Apr to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.444620329Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189016350017115c",
    "gate_id": "F-CONV-G-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Convert Gguf to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.447195682Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901635004219c9",
    "gate_id": "F-CONV-G-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Convert Gguf to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.450017269Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "0000000000000000189016350068df6a",
    "gate_id": "F-CONV-S-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Convert SafeTensors to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.452557048Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901635008ed9e1",
    "gate_id": "F-CONV-S-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Convert SafeTensors to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.455046020Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890163500bced76",
    "gate_id": "F-CONV-A-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Convert Apr to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.458068031Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890163500e727ca",
    "gate_id": "F-CONV-A-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Convert Apr to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.460832972Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901635010fc4af",
    "gate_id": "F-CONV-S-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Convert SafeTensors to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.463494366Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901635013e1b29",
    "gate_id": "F-CONV-S-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Convert SafeTensors to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.466533634Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890163501928697",
    "gate_id": "F-CONV-RT-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Round-trip conversion",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Round-trip failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.472063847Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890163501e1e7c7",
    "gate_id": "F-CONV-RT-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Round-trip conversion",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Round-trip failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.477265741Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018901635020d7d5b",
    "gate_id": "F-GOLDEN-RULE-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Golden Rule: convert  inference  diff",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Golden Rule: original inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T09:44:30.480125015Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  }
]