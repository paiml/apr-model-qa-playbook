[
  {
    "id": "000000000000000018902c4744d3412d",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 14256.03ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 13156.6ms (2.4 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.5,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 14262
    },
    "timestamp": "2026-02-01T16:28:58.165604082Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c528b804d23",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000001",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 1,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 5\nlatency: 48415.03ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\nGenerated 32 tokens in 42156.9ms (0.8 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.1,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 48430
    },
    "timestamp": "2026-02-01T16:29:46.595990389Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c537dbf3e36",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_safetensors_0000000000000002",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 2,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 4064
    },
    "timestamp": "2026-02-01T16:29:50.660202229Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c56c276bbb3",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000003",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 3,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 14030.36ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 13169.3ms (2.4 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.6,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 14037
    },
    "timestamp": "2026-02-01T16:30:04.697979346Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c61fcd0cd6e",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_apr_0000000000000004",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 4,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 5\nlatency: 48207.68ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\nGenerated 32 tokens in 42035.7ms (0.8 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.1,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 48223
    },
    "timestamp": "2026-02-01T16:30:52.921601641Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c62d6025298",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_safetensors_0000000000000005",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 5,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 3643
    },
    "timestamp": "2026-02-01T16:30:56.565502606Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c661c0d36c7",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_cpu_gguf_0000000000000006",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 6,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 14052.28ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 13177.1ms (2.4 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.6,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 14059
    },
    "timestamp": "2026-02-01T16:31:10.625523139Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c718a6ef9d0",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_cpu_apr_0000000000000007",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 7,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 5\nlatency: 49081.54ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\nGenerated 32 tokens in 42676.3ms (0.7 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.1,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 49096
    },
    "timestamp": "2026-02-01T16:31:59.722064606Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c72620e136e",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_cpu_safetensors_0000000000000008",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 8,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 3617
    },
    "timestamp": "2026-02-01T16:32:03.339592829Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c75be1cf722",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_gpu_gguf_0000000000000009",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 9,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 14422.48ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 13558.4ms (2.4 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.5,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 14429
    },
    "timestamp": "2026-02-01T16:32:17.768973674Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c8117aafffe",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_gpu_apr_000000000000000a",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 10,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 5\nlatency: 48732.91ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\nGenerated 32 tokens in 42622.0ms (0.8 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.1,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 48747
    },
    "timestamp": "2026-02-01T16:33:06.516094884Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c81efe50ee4",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_chat_gpu_safetensors_000000000000000b",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 11,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 3627
    },
    "timestamp": "2026-02-01T16:33:10.143779073Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c853e6e5b4e",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_cpu_gguf_000000000000000c",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 12,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 14194.04ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 13330.9ms (2.4 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.5,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 14202
    },
    "timestamp": "2026-02-01T16:33:24.346300662Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c908ad63d1e",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_cpu_apr_000000000000000d",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 13,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 5\nlatency: 48511.59ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\nGenerated 32 tokens in 42466.4ms (0.8 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.1,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 48526
    },
    "timestamp": "2026-02-01T16:34:12.872818141Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c91661f0f2f",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_cpu_safetensors_000000000000000e",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 14,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 3678
    },
    "timestamp": "2026-02-01T16:34:16.551800753Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c94b3d6bdc5",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_gpu_gguf_000000000000000f",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 15,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf\n\ntokens: 22\nlatency: 14181.40ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 13297.5ms (2.4 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.6,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 14188
    },
    "timestamp": "2026-02-01T16:34:30.740585675Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902c9ff3f900f4",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_gpu_apr_0000000000000010",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 16,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr\n\ntokens: 5\nlatency: 48305.06ms\nmodel: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\n[AprV2ModelCuda] Pre-cached 3155 MB of weights on GPU (24 layers, 0 quantized, 264 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 519 MB\n[PMAT-171] Loaded embedded BPE tokenizer: 151643 vocab, 151387 merges, 3 special tokens\nGenerated 32 tokens in 41979.8ms (0.8 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.1,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 48320
    },
    "timestamp": "2026-02-01T16:35:19.061213148Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d207b391",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_serve_gpu_safetensors_0000000000000011",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 17,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with either name: 'model.norm.weight' or 'output_norm.weight'\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/pacha/models/qwen2-5-coder-0-5b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 3725
    },
    "timestamp": "2026-02-01T16:35:22.786718349Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d234d1d1",
    "gate_id": "F-CONV-G-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Convert Gguf to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.789674562Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d25f62da",
    "gate_id": "F-CONV-G-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Convert Gguf to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.792462209Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d2851b5f",
    "gate_id": "F-CONV-A-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Convert Apr to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.794934118Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d2a80791",
    "gate_id": "F-CONV-A-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Convert Apr to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.797222769Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d2cbeb4a",
    "gate_id": "F-CONV-G-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Convert Gguf to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.799574836Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d2ef28c8",
    "gate_id": "F-CONV-G-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Convert Gguf to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.801884588Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d3223aca",
    "gate_id": "F-CONV-S-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Convert SafeTensors to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.805233557Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d34bfbcf",
    "gate_id": "F-CONV-S-G",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Convert SafeTensors to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.807968064Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d375eee8",
    "gate_id": "F-CONV-A-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Convert Apr to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.810716652Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d39d1d14",
    "gate_id": "F-CONV-A-S",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Convert Apr to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.813284786Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d3c3736a",
    "gate_id": "F-CONV-S-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Convert SafeTensors to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.815797056Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d3e71a2f",
    "gate_id": "F-CONV-S-A",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Convert SafeTensors to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: unexpected argument '--gpu' found\n\n  tip: to pass '--gpu' as a value, use '-- --gpu'\n\nUsage: apr run --prompt <PROMPT> --max-tokens <MAX_TOKENS> <SOURCE>\n\nFor more information, try '--help'.\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.818133639Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d440ad86",
    "gate_id": "F-CONV-RT-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Round-trip conversion",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Round-trip failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.824005063Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d49a78e2",
    "gate_id": "F-CONV-RT-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Round-trip conversion",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Round-trip failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.829888996Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "000000000000000018902ca0d4bf6e25",
    "gate_id": "F-GOLDEN-RULE-001",
    "scenario": {
      "id": "Qwen2.5-Coder-0.5B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-0.5B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Golden Rule: convert → inference → diff",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Golden Rule: original inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-01T16:35:22.832310498Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  }
]