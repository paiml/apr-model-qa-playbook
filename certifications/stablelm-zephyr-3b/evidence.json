[
  {
    "id": "00000000000000001893d4afa81357ce",
    "gate_id": "G0-PULL-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Pull: acquire model via apr pull",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: model acquired via apr pull\n\u001b[1;36m=== APR Pull ===\u001b[0m\n\nModel: \u001b[36mhf://stabilityai/stablelm-zephyr-3b/model.safetensors\u001b[0m\n\n\u001b[33mDownloading...\u001b[0m\n\r  [==================================================] 100.0% (0 B/0 B)\r  [==================================================] 100.0% (5.21 GB/5.21 GB)\n\n\u001b[32m✓\u001b[0m Downloaded successfully\n  Path: \u001b[32m/home/noah/.cache/pacha/models/900238ffddac27b8.safetensors\u001b[0m\n  Size: \u001b[33m5.21 GB\u001b[0m\n  Format: SafeTensors(SafeTensorsInfo { tensor_count: 356, tensors: {\"model.layers.9.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.27.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.4.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.12.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.1.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.19.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.4.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.2.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.30.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.norm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"lm_head.weight\": TensorInfo { shape: [50304, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.7.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.12.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.12.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.12.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.12.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.15.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.12.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.12.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.21.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.15.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.12.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.4.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.4.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.22.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.19.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.17.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.7.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.3.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.7.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.10.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.4.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.12.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.9.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.10.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.4.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.17.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.4.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.4.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.28.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.12.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.15.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.7.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.20.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.norm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.25.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.6.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.0.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.12.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.24.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.3.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.17.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.10.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.4.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.18.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.6.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.14.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.27.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.1.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.5.input_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.embed_tokens.weight\": TensorInfo { shape: [50304, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.26.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.30.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.4.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.16.input_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.post_attention_layernorm.bias\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.29.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.31.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.4.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.22.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.21.mlp.down_proj.weight\": TensorInfo { shape: [2560, 6912], dtype: \"BF16\", offset: 0 }, \"model.layers.14.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.23.self_attn.o_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.11.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }, \"model.layers.8.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.28.self_attn.q_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.mlp.gate_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.19.self_attn.v_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.27.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.9.self_attn.k_proj.weight\": TensorInfo { shape: [2560, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.2.mlp.up_proj.weight\": TensorInfo { shape: [6912, 2560], dtype: \"BF16\", offset: 0 }, \"model.layers.13.post_attention_layernorm.weight\": TensorInfo { shape: [2560], dtype: \"BF16\", offset: 0 }}, metadata: {\"format\": \"pt\"}, parameters: Some(2795443200), dtype: Some(\"BF16\") })\n  Hash: 900238ffddac27b8\n  \u001b[32m✓\u001b[0m 900238ffddac27b8.tokenizer.json (\u001b[2m2.02 MB\u001b[0m)\n  \u001b[32m✓\u001b[0m 900238ffddac27b8.config.json (\u001b[2m599 B\u001b[0m)\n\n\u001b[1;36mUsage:\u001b[0m\n  apr run /home/noah/.cache/pacha/models/900238ffddac27b8.safetensors\n  apr serve /home/noah/.cache/pacha/models/900238ffddac27b8.safetensors\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 153557
    },
    "timestamp": "2026-02-13T14:28:49.390962258Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d4caad249851",
    "gate_id": "G0-FORMAT-APR-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_apr_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "G0 Format: prepare Apr workspace",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: converted to apr\n\u001b[1;36m=== Rosetta Stone Conversion ===\u001b[0m\n\nSource: /home/noah/.cache/pacha/models/900238ffddac27b8.safetensors\nTarget: output/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr\n\n\u001b[33m--- Source Inspection ---\u001b[0m\n╭────────────┬───────────────╮\n│     Format │ SafeTensors   │\n├────────────┼───────────────┤\n│  File Size │ 5.21 GiB      │\n│    Tensors │ 356           │\n│ Parameters │ 2,795,443,200 │\n╰────────────┴───────────────╯\n\n\u001b[33mConverting...\u001b[0m\n\n\u001b[33m--- Target Inspection ---\u001b[0m\n╭──────────────┬───────────────╮\n│       Format │ APR           │\n├──────────────┼───────────────┤\n│    File Size │ 10.41 GiB     │\n│      Tensors │ 356           │\n│   Parameters │ 2,795,443,200 │\n│ Architecture │ llama         │\n╰──────────────┴───────────────╯\n\n\u001b[1;36m=== Conversion Summary ===\u001b[0m\nPath: SafeTensors → APR\nDuration: 97925ms\nTensors: 356 -> 356\n\n\u001b[1;32mConversion successful\u001b[0m\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 116046
    },
    "timestamp": "2026-02-13T14:30:45.440092882Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d51f40d1c9fb",
    "gate_id": "G0-FORMAT-GGUF-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "G0 Format: prepare Gguf workspace",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: converted to gguf\n\u001b[1;36m=== Rosetta Stone Conversion ===\u001b[0m\n\nSource: /home/noah/.cache/pacha/models/900238ffddac27b8.safetensors\nTarget: output/workspace/stabilityai/stablelm-zephyr-3b/gguf/model.gguf\n\n\u001b[33m--- Source Inspection ---\u001b[0m\n╭────────────┬───────────────╮\n│     Format │ SafeTensors   │\n├────────────┼───────────────┤\n│  File Size │ 5.21 GiB      │\n│    Tensors │ 356           │\n│ Parameters │ 2,795,443,200 │\n╰────────────┴───────────────╯\n\n\u001b[33mConverting...\u001b[0m\n\n\u001b[33m--- Target Inspection ---\u001b[0m\n╭──────────────┬───────────────╮\n│       Format │ GGUF          │\n├──────────────┼───────────────┤\n│    File Size │ 1.88 GiB      │\n│      Tensors │ 356           │\n│   Parameters │ 2,795,443,200 │\n│ Architecture │ llama         │\n│ Quantization │ 12            │\n╰──────────────┴───────────────╯\n\n\u001b[1;36m=== Conversion Summary ===\u001b[0m\nPath: SafeTensors → GGUF\nDuration: 334994ms\nTensors: 356 -> 356\n\n\u001b[1;32mConversion successful\u001b[0m\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 363212
    },
    "timestamp": "2026-02-13T14:36:48.694933941Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d523bce8469f",
    "gate_id": "G0-VALIDATE-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Validate: NaN/Inf/all-zeros tensor check",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: model.safetensors physics validated\nValidating output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/model.safetensors...\n\n\n\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n  \u001b[1;36mValidate: SafeTensors (Rosetta Stone)\u001b[0m\n\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n╭─────────────────────────────────────────────────┬──────────────────────────┬──────────╮\n│ Tensor                                          │ Status                   │ Failures │\n├─────────────────────────────────────────────────┼──────────────────────────┼──────────┤\n│ lm_head.weight                                  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.embed_tokens.weight                       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.input_layernorm.bias            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.mlp.gate_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.mlp.up_proj.weight              │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.post_attention_layernorm.bias   │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.self_attn.k_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.self_attn.q_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.self_attn.v_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.input_layernorm.bias             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.mlp.gate_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.mlp.up_proj.weight               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.post_attention_layernorm.bias    │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.self_attn.k_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.self_attn.q_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.self_attn.v_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.norm.bias                                 │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.norm.weight                               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n╰─────────────────────────────────────────────────┴──────────────────────────┴──────────╯\n\nVALID: 356 tensors checked, 0 contract violations (PMAT-235)\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 19260
    },
    "timestamp": "2026-02-13T14:37:07.956710428Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d523bcf45de4",
    "gate_id": "G0-INTEGRITY-CONFIG",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Integrity: config.json vs tensor metadata",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: config.json matches tensor metadata",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T14:37:07.957440413Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d523bd2ffd9c",
    "gate_id": "G0-LAYOUT-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Layout: tensor shape contract validation",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: Tensor layouts conform to contract\n  Rules checked: 292\n  Rules passed: 292",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T14:37:07.961348115Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d533ce7c5de4",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/model.safetensors\n\n\u001b[1;36m=== Benchmark Results ===\u001b[0m\ntokens: 32\nlatency: 51511.20ms\nmodel: output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/model.safetensors",
    "stderr": "[GH-189] Loaded tokenizer from output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/tokenizer.json: 25 special tokens\n\u001b[32mGenerated 32 tokens in 28338.2ms (1.1 tok/s)\u001b[0m\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.6,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 69009
    },
    "timestamp": "2026-02-13T14:38:16.971044229Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d55e94fad957",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_apr_0000000000000001",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 1,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr\n\n\u001b[1;36m=== Benchmark Results ===\u001b[0m\ntokens: 32\nlatency: 134096.38ms\nmodel: output/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 50304 vocab, 50009 merges, 3 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[50304, 2560], expected [vocab=50304, hidden=2560]\n[APR-LOAD] Embedding dims=[50304, 2560], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [0.0013, 0.0004, 0.0005, 0.0034, 0.0022]\n[APR-LOAD] Embedding loaded: 128778240 elements (vocab=50304 x hidden=2560)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[50304, 2560], dtype=0, expected [vocab=50304, hidden=2560]\n[APR-LOAD] LM head loaded: 128778240 elements (hidden=2560 x vocab=50304)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\n[PMAT-171] Loaded embedded BPE tokenizer: 50304 vocab, 50009 merges, 3 special tokens\n\u001b[32mGenerated 32 tokens in 45650.8ms (0.7 tok/s)\u001b[0m\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 183718
    },
    "timestamp": "2026-02-13T14:41:20.689852113Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d56511835846",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_gguf_0000000000000002",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 2,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/stabilityai/stablelm-zephyr-3b/gguf/model.gguf\n\n\u001b[1;36m=== Benchmark Results ===\u001b[0m\ntokens: 32\nlatency: 6408.03ms\nmodel: output/workspace/stabilityai/stablelm-zephyr-3b/gguf/model.gguf",
    "stderr": "\u001b[32mGenerated 32 tokens in 4109.7ms (7.8 tok/s)\u001b[0m\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 5.0,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 27859
    },
    "timestamp": "2026-02-13T14:41:48.548972208Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d56edd716f38",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_gpu_safetensors_0000000000000003",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 3,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/model.safetensors\n\n\u001b[1;36m=== Benchmark Results ===\u001b[0m\ntokens: 32\nlatency: 23336.62ms\nmodel: output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/model.safetensors",
    "stderr": "[GH-189] Loaded tokenizer from output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/tokenizer.json: 25 special tokens\n\u001b[32mGenerated 32 tokens in 2197.3ms (14.6 tok/s)\u001b[0m\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.4,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 42076
    },
    "timestamp": "2026-02-13T14:42:30.625056252Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d58b9b86ddd8",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_gpu_apr_0000000000000004",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 4,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr\n\n\u001b[1;36m=== Benchmark Results ===\u001b[0m\ntokens: 32\nlatency: 77563.23ms\nmodel: output/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 50304 vocab, 50009 merges, 3 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[50304, 2560], expected [vocab=50304, hidden=2560]\n[APR-LOAD] Embedding dims=[50304, 2560], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [0.0013, 0.0004, 0.0005, 0.0034, 0.0022]\n[APR-LOAD] Embedding loaded: 128778240 elements (vocab=50304 x hidden=2560)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[50304, 2560], dtype=0, expected [vocab=50304, hidden=2560]\n[APR-LOAD] LM head loaded: 128778240 elements (hidden=2560 x vocab=50304)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\n[PMAT-171] Loaded embedded BPE tokenizer: 50304 vocab, 50009 merges, 3 special tokens\n\u001b[32mGenerated 32 tokens in 33570.6ms (1.0 tok/s)\u001b[0m\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.4,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 123448
    },
    "timestamp": "2026-02-13T14:44:34.073216728Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d591b14c9650",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_gpu_gguf_0000000000000005",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 5,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/stabilityai/stablelm-zephyr-3b/gguf/model.gguf\n\n\u001b[1;36m=== Benchmark Results ===\u001b[0m\ntokens: 32\nlatency: 6115.86ms\nmodel: output/workspace/stabilityai/stablelm-zephyr-3b/gguf/model.gguf",
    "stderr": "\u001b[32mGenerated 32 tokens in 3772.2ms (8.5 tok/s)\u001b[0m\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 5.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 26135
    },
    "timestamp": "2026-02-13T14:45:00.208299124Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d643ef92cc8c",
    "gate_id": "G3-STABLE",
    "scenario": {
      "id": "stablelm-zephyr-3b_chat_cpu_safetensors_0000000000000006",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 6,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Crashed",
    "reason": "Process crashed with exit code -1",
    "output": "",
    "stderr": "\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled for chat (APR-TRACE-001)\u001b[0m\n  Trace level: basic\n[APR-TRACE] Formatted prompt (12 chars):\n[APR-TRACE] \"What is 2+2?\"\n[APR-TRACE] Prompt tokens (6 tokens): [1276, 310, 374, 12, 19, 32]\n\n--- TRACE STDOUT ---\n\n\u001b[1;36m=== Model Chat (SafeTensors Format) ===\u001b[0m\n\n\u001b[36mUsing SafeTensors with mmap (Native Library Mandate)\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/model.safetensors\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m SafeTensors format in 2.66s (5590.9 MB)\n\u001b[32mLoaded tokenizer:\u001b[0m output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/tokenizer.json (\u001b[2m151936 tokens\u001b[0m)\n\u001b[32mLoaded\u001b[0m config: 32 layers, 2560 hidden, 32 heads\n\u001b[32mDetected\u001b[0m \u001b[36mRaw\u001b[0m chat template\n\u001b[92m[SafeTensors CUDA: NVIDIA GeForce RTX 4090 (24045 MB VRAM) — pre-cached]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m exports Shepherd meatexportsС\")] meat Shepherd meat meat meat meat meat meatinerinerinerinerinerinerinerinerinerinerωinerωinerωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωώώωωωωωωωωωωωωωωωωωωωωωωωωωω Continueчωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdωωωωω Shepherdω‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Shepherd‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Bened Shepherd‑ Bened Shepherd‑ Bened‑ Bened‑ Bened‑ Bened‑ Bened‑ Bened‑ Shepherd‑ Shepherd‑ Bened‑ Bened‑ Shepherd‑ Shepherd‑ Shepherd‑ Bened‑ Bened Shepherd‑ Bened Shepherd‑ Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened‑ Bened‑ Bened Bened Bened Bened Bened Bened Bened‑ Bened‑ Bened‑ Bened‑ Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened‑ Bened‑ Bened‑ Bened‑ Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Hebrew Hebrew Hebrew Hebrew Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m\n",
    "exit_code": -1,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 765548
    },
    "timestamp": "2026-02-13T14:57:45.757310115Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6bfefc46ecf",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_chat_cpu_apr_0000000000000007",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 7,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (APR Format) ===\u001b[0m\n\n\u001b[36mUsing APR v2 format with mmap (Native Library Mandate)\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m APR format in 11.14s (11182.9 MB)\n\u001b[32mLoaded tokenizer from HuggingFace cache:\u001b[0m /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json (\u001b[2m151936 tokens\u001b[0m)\n\u001b[32mDetected\u001b[0m \u001b[36mLLaMA2\u001b[0m chat template\n\u001b[92m[APR CUDA: NVIDIA GeForce RTX 4090 (24045 MB VRAM) — pre-cached]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m gap gap gap gap gap gap gap gap gap gapicleicleicleicleicleicleicleicleicleicleicle\");\n\");\n\");\nanasanasanasanasanasasionasionCOLOR CRMicleCOLOR switch menosicle switch.GroupLayout menos menos menos menos menos menos menos menos menos menos menos menos_relation_relation_relation_relation_relation_relation menos menos menos menos menos_relation_relation_relation_relation_relation_relation menos menos_relation_relation_relation_relation_relation_relation_relation_relation_relation_relation_relation_relation_relation_relation_relation_relation menos menos menos_relation_relation_relation_relation_relation_relation_relation_relation menos menos menos menos menos menos menos menos menos menos menos_relation_relation_relation_relation menos_relation menos_relation_relation_relation_relation_relation menos menos menos menos menos menos menos menos menos menos menos menos menos menos menos menos menos\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t menos menos menos menos menos menos menos\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t acad acad acad acad acad acad acad\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t acad acad acad acad acad acad acad acad acad\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t acad acad acad acad\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t acad acad acad acad\t\t\t\t\t\t\t acad acad acad Count Count Count Count acad acad acad acad acad acad acad acad acad Count Count Count Count Count Count Count Count Count acad acad Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count acad acad acad acad acad acad acad acad acad Count Count Count acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad acad!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!:!: Count Count Count Count Count Count Count!:!:!:!:!:!: Count Count Count Count Count Count Count!:!:!: Count Count Count Count!:!:!: Count Count Count!:!:!:!:!:!:!:!:!: Count Count Count Count Count!:!:!:!:!:!:!:!:!:!:!:!:!: Count Count Count Count!:!:!:!:!:!:!: Count Count Count Count Count Count!:!:!:!:!:!:!: Count Count Count Count Count Count Count Count Count Count!:!:!: Count Count Count Count Count Count Count Count Count Count Count!:!:!:!:!:!:!:!:!: Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count Count\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": "[AprV2ModelCuda] VRAM sufficient (23159 MB free), using full cache mode\n[AprV2ModelCuda] Pre-cached 10171 MB of weights on GPU (32 layers, 0 quantized, 224 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 491 MB\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[50304, 2560], expected [vocab=50304, hidden=2560]\n[APR-LOAD] Embedding dims=[50304, 2560], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [0.0013, 0.0004, 0.0005, 0.0034, 0.0022]\n[APR-LOAD] Embedding loaded: 128778240 elements (vocab=50304 x hidden=2560)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[50304, 2560], dtype=0, expected [vocab=50304, hidden=2560]\n[APR-LOAD] LM head loaded: 128778240 elements (hidden=2560 x vocab=50304)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 532578
    },
    "timestamp": "2026-02-13T15:06:38.336467180Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6d03178fcd4",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_chat_cpu_gguf_0000000000000008",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 8,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (GGUF Format) ===\u001b[0m\n\n\u001b[36mUsing GGUF format with realizar inference engine\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/stabilityai/stablelm-zephyr-3b/gguf/model.gguf\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m GGUF format in 0.91s (2017.8 MB)\n\u001b[32mLoaded\u001b[0m tokenizer with 50304 tokens\n\u001b[32mDetected\u001b[0m \u001b[36mLLaMA2\u001b[0m chat template\n\u001b[33m[GGUF CUDA init failed: Inference error: PARITY-GATE FAILED: GPU computes a DIFFERENT function than CPU.\n\nCosine similarity: 0.730927 (required: ≥0.99)\nCPU argmax: 6350 | GPU argmax: 6350\nMax absolute logit difference: 9.1004\n\nThis model's dimensions (hidden=2560, heads=40, kv_heads=40) cause\nGPU forward pass to diverge from CPU. The GPU CANNOT serve this model.\n\nRun `apr parity <model>` for full SPC diagnosis.\nSet SKIP_PARITY_GATE=1 to bypass (for debugging only)., will use CPU]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m gioga Offer Offer mutualpiredmut intentionalga Sto Stoaintedrivesdaldaliangiangு infectedrivesுaintedious Ec Ec simultaneousাiousainted intentionalga Ecrivesுাriveundauablefected Liverdalarlydaldalarlyainteddalaintedroveuableাdalাাaught transmittedrivesাdaltawaাা transmittedা Gentیtawa Ministaintedroveworthy Gentা�tawafectedworthy��াা�dalaintedাdal Gentainted Gentdalouncingomaaintedmansungnaments Gentainted�reekা simultaneousarlyouncingaintedাaintedা behalf Gent Gent behalfaintedainted�arlyাouncing behalfাাarly����া Gentাাা Gent�াা� Gent spreadingাrive Gentdal� spreadingা Gent behalfাা���arly�arlydalাarlyা Gentarly���� behalf�া Nob��া Ministা� behalfাে behalfnaments Nob Nobাarly��াা Nob Gent Gentreng Gent�াাা� Gentাাarly�া behalf� Ministা��াাাাো Gentmarginাাা Gentoma�াা�া Para�ে Gent Ru behalfreek� behalf Gent behalfাে Gent behalf��া Ministাাworthy道াাাা behalfাگgent behalf�া behalfে behalfreek Gamb��道াা� Gentasperrengাা behalf��rengা Ministা�্া behalf� knowingly behalf��াাাা behalf�াা behalf্� spreadingো��� behalfাাাাাাা�rengাাreng��া Bened kindness Gentی��া��undersাা��� behalf�া behalfা�া্ Gent�া Gent�াা behalfা�� Gent��া Paraা Paraা�া behalfা behalfা behalfাা behalf behalf behalf behalf behalf behalf behalf� Gentenigা��undersা��� Ministা� behalf behalf�া্ Gent behalf Gentা behalfাে�� Benedoma Benedাendar Bened behalf�া Benedgettা behalfarlyা�া��া behalf�� behalf behalf behalf behalf�enig behalf behalfrengা behalf behalf behalfোা behalfা behalf�� behalf behalf�����া behalf� behalf behalf behalf behalfা behalf behalf behalf�াা����reek behalf behalfoma behalf behalf�� Gov�� behalfা����া� behalf behalf�gent behalf behalf behalf����\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 69821
    },
    "timestamp": "2026-02-13T15:07:48.158293389Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6dfa6813f39",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_chat_gpu_safetensors_0000000000000009",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 9,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (SafeTensors Format) ===\u001b[0m\n\n\u001b[36mUsing SafeTensors with mmap (Native Library Mandate)\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/model.safetensors\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m SafeTensors format in 2.33s (5590.9 MB)\n\u001b[32mLoaded tokenizer:\u001b[0m output/workspace/stabilityai/stablelm-zephyr-3b/safetensors/tokenizer.json (\u001b[2m151936 tokens\u001b[0m)\n\u001b[32mLoaded\u001b[0m config: 32 layers, 2560 hidden, 32 heads\n\u001b[32mDetected\u001b[0m \u001b[36mRaw\u001b[0m chat template\n\u001b[92m[SafeTensors CUDA: NVIDIA GeForce RTX 4090 (24045 MB VRAM) — pre-cached]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m meat Shepherd meat meat meat meat meat meatinerinerinerinerinerinerinerinerinerinerωinerωinerωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωώώωωωωωωωωωωωωωωωωωωωωωωωωωω Continueчωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωωω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdω Shepherdωωωωω Shepherdω‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Shepherd‑ Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd Shepherd‑ Bened Shepherd‑ Bened Shepherd‑ Bened‑ Bened‑ Bened‑ Bened‑ Bened‑ Bened‑ Shepherd‑ Shepherd‑ Bened‑ Bened‑ Shepherd‑ Shepherd‑ Shepherd‑ Bened‑ Bened Shepherd‑ Bened Shepherd‑ Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened‑ Bened‑ Bened Bened Bened Bened Bened Bened Bened‑ Bened‑ Bened‑ Bened‑ Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened‑ Bened‑ Bened‑ Bened‑ Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Hebrew Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Hebrew Hebrew Hebrew Hebrew Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened Bened\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 66387
    },
    "timestamp": "2026-02-13T15:08:54.546278146Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d76d0e347ae0",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_chat_gpu_apr_000000000000000a",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 10,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (APR Format) ===\u001b[0m\n\n\u001b[36mUsing APR v2 format with mmap (Native Library Mandate)\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m APR format in 6.40s (11182.9 MB)\n\u001b[32mLoaded tokenizer from HuggingFace cache:\u001b[0m /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json (\u001b[2m151936 tokens\u001b[0m)\n\u001b[32mDetected\u001b[0m \u001b[36mLLaMA2\u001b[0m chat template\n\u001b[92m[APR CUDA: NVIDIA GeForce RTX 4090 (24045 MB VRAM) — pre-cached]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m icleicleicleicleicleicleicleicleicle��� Adapter Adapter Adapter Adapter Adapter Adapter Adapter AdapterComputerComputer>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\nManifestManifest>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n>\n\n\n\n cards excitement excitement excitement excitement cards excitement excitement excitement excitement excitement cards cards cards excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement_fd_fd excitement excitement excitement excitement excitement_fd_fd excitement excitement excitement excitement excitement excitement excitement excitement excitement_fd_fd_fd excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement excitement\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": "[AprV2ModelCuda] VRAM sufficient (13130 MB free), using full cache mode\n[AprV2ModelCuda] Pre-cached 10171 MB of weights on GPU (32 layers, 0 quantized, 224 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 491 MB\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 607330
    },
    "timestamp": "2026-02-13T15:19:01.876466896Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d77d9784f042",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_chat_gpu_gguf_000000000000000b",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 11,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (GGUF Format) ===\u001b[0m\n\n\u001b[36mUsing GGUF format with realizar inference engine\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/stabilityai/stablelm-zephyr-3b/gguf/model.gguf\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m GGUF format in 0.94s (2017.8 MB)\n\u001b[32mLoaded\u001b[0m tokenizer with 50304 tokens\n\u001b[32mDetected\u001b[0m \u001b[36mLLaMA2\u001b[0m chat template\n\u001b[33m[GGUF CUDA init failed: Inference error: PARITY-GATE FAILED: GPU computes a DIFFERENT function than CPU.\n\nCosine similarity: 0.730927 (required: ≥0.99)\nCPU argmax: 6350 | GPU argmax: 6350\nMax absolute logit difference: 9.1004\n\nThis model's dimensions (hidden=2560, heads=40, kv_heads=40) cause\nGPU forward pass to diverge from CPU. The GPU CANNOT serve this model.\n\nRun `apr parity <model>` for full SPC diagnosis.\nSet SKIP_PARITY_GATE=1 to bypass (for debugging only)., will use CPU]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m mutual mutual mutual mutual mutualious mutualiang mutualious mutualworthyrives mutualScal mutualrives mutualious mutualiousAnt Admindaliousious Liveriftsriveiousiousious Ecộiousiousộiousrishiousious intentional Ecộious Liveriousiousaughtiousộ Liverious:& Eciousiftsumbledmansious Eciousious Gentrishiousious Eciousiftsiousộiousiousiousiousieveiousiousributes Gent intentional intentionalEcmansouncingiftsiftsmansributesiousmansộmansiousộmansiftsmansộmansộrivers Gent Liver widmansouncingộộBell (& Ecộworthyributesumbled beautious Gentifts intentionalVMributesiousাiftsiftsা wid amplify Ecmansmansworthynamentsious beautualiftsiousnamentsmansiousmansualiftsmansাedgesộmansmansộা exposeাifts GentộVersmansiousiftsmansious Gentimpleiousộmansmansmans Gentumblediftsifts intentionalmansộ occasion (&mans shadeumbledmans amplifyাiftsiftsmansiftsVersumbledworthyVers Gentmansordingouncingmansiftsmansmans Gentiftsmans manifold Gamb Gentributesmans Gent Gentmansộ imperfect intentionaliftsgififtsorneiftsworthyious Gent Gentộ Gentাộifts Gentmans Gent Gentifts Gentmans Gent manifold amplify amplmansouncemansmansmans Gent GentmansVers amplifymansVers Gentmans Gent Gent Gent wid)] Ecmans mansmans Ecộ Gent:&)]iftsাOX (&mans Gentmansা Gentা Gentmansmans Gent manifold amplify)]mans)]mansifts Gentmansmansriers Gent Gentmansmans amplify (& Gent manifold Gentmans Gent Gentaught Gentnamentsworthyiftsmans Gent Gentworthy Gent wound GentVers Gent Gent”:mans EcOX Gent Gentiftsmansmans)]rivers Gent Gent Gentmansiftsmans Gent strokes Gent manifold Gent Gentnaments Gentati mans consc Gentiftsgifworthy Gent manifold consc intentional unw Gentnaments (& Gent (& Gent Gent Gent Gent Gentifts widmansouncing”:Vers Gent Gent Gent Gent Gent Gentা woundộা”:ộ Gent manifoldmansiftsmans Gent Gentmansorneifts Gent GentOXmansgygif”:mansmansworthymans'_mans Gent Gent consc Gent Gent manifold GentriersাVers imperfect Gent (&namentsifts woundmans Gent Gentifts Gent Gent Gent Gent Gent Gent Gent Gent consc Gentmansumbledmans Gentmansifts wid Gent Gent Gentmansmans Gentmansatiayermans (& Gent (&vaা Gent Gent conscifts Gent consc Gent Gent Gent Genterrors consc Gent Gent Gentworthy Gentounceাgifumbled Gent GentmansmansmansatiVers�ifts Gentnaments Gent Editioniftsati”: imperfect Gentmans conscienceimpleatiriers Gent Gent unwatiifts\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 71023
    },
    "timestamp": "2026-02-13T15:20:12.899696266Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d78d830e3ab7",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_serve_cpu_safetensors_000000000000000c",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 12,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "{\"latency_ms\":30275,\"text\":\"!!!!!!!\",\"tok_per_sec\":1.0569443264412897,\"tokens_generated\":32}",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 68376
    },
    "timestamp": "2026-02-13T15:21:21.275847526Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d79750e4179c",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_serve_cpu_apr_000000000000000d",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 13,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 42107
    },
    "timestamp": "2026-02-13T15:22:03.383898438Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7a48f46431a",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_serve_cpu_gguf_000000000000000e",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 14,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "{\"token_ids\":[1276,310,374,12,19,32,1001,21086,33871,21086,6147,6147,21086,38775,48554,27497,11150,21086,46516,21086,21086,21086,8837,21086,21086,21086,21086,21086,21086,21086,21086,21086,21086,21086,21086,21086,21086,21086],\"text\":\"What is 2+2?ield caut cinnamon cautyryr caut fen pastureinj progn caut Dios caut caut caut tumors caut caut caut caut caut caut caut caut caut caut caut caut caut caut caut\",\"num_generated\":32}",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 56881
    },
    "timestamp": "2026-02-13T15:23:00.265093448Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7a5072307bd",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_serve_gpu_safetensors_000000000000000f",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 15,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 2010
    },
    "timestamp": "2026-02-13T15:23:02.276050552Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7b03e02293b",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_serve_gpu_apr_0000000000000010",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 16,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 48165
    },
    "timestamp": "2026-02-13T15:23:50.441284335Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7bf1089e3f3",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_serve_gpu_gguf_0000000000000011",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 17,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "{\"token_ids\":[1276,310,374,12,19,32,21935,5600,9132,6147,33871,42290,6147,13853,10295,42290,49032,47696,9102,48554,48554,48554,23462,23462,15403,4867,8837,2247,22562,48554,18888,10295,18888,4502,22562,24030,23756,23462],\"text\":\"What is 2+2? seizure transl meatyr cinnamonchidyracles kernelchidwine Meat)] pasture pasture pasture chambers chambers membranes Tw tumorsamentkernel pastureacular kernelacular tumorkernel deficientexports chambers\",\"num_generated\":32}",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 63661
    },
    "timestamp": "2026-02-13T15:24:54.102937149Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7ef0d43c819",
    "gate_id": "F-GOLDEN-RULE-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_apr_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Golden Rule: convert → inference → diff",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Golden Rule PASS: identical output: ",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:28:20.206439506Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7f36aa8a210",
    "gate_id": "F-CONTRACT-I2-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_apr_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Format contract invariant",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "I-2 Tensor Name Bijection: all 0 source tensors present in APR (0 total)",
    "output": "source=0, apr=0",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:28:38.953198827Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7fe2155b8d2",
    "gate_id": "F-CONTRACT-I3-001",
    "scenario": {
      "id": "stablelm-zephyr-3b_run_cpu_apr_0000000000000000",
      "model": {
        "org": "stabilityai",
        "name": "stablelm-zephyr-3b",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Format contract invariant",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "I-3 No Silent Fallbacks: no F32 fallbacks detected",
    "output": "\n\u001b[1;36m=== Model Self-Test (PMAT-112: Real Validation) ===\u001b[0m\nModel: \u001b[36moutput/workspace/stabilityai/stablelm-zephyr-3b/apr/model.apr\u001b[0m\n\n┌─────┬─────────────────────┬──────────────────────────────────────┬──────┐\n│  #  │      Component      │               Details                │ Pass │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 1   │ Tokenizer           │ tokens=[1, 2]                        │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 2   │ Embedding           │ Found embedding tensor               │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 3   │ Positional Encoding │ RoPE computed inline                 │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 4   │ Q/K/V Projection    │ Q/K/V found                          │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 5   │ Attention Scores    │ Attention output found               │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 6   │ Feed-Forward (MLP)  │ MLP found                            │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 7   │ Layer Norm          │ 32 layers                            │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 8   │ LM Head             │ vocab_size=50304                     │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 9   │ Logits → Probs      │ logits[50304]                        │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 10  │ Sampler/Decode      │ softmax sum = 1.000014               │ \u001b[32m✅   \u001b[0m │\n└─────┴─────────────────────┴──────────────────────────────────────┴──────┘\n\n\u001b[1;32m✅ 10/10 STAGES PASSED. MODEL PROVEN CORRECT.\u001b[0m\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:29:24.967669134Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  }
]