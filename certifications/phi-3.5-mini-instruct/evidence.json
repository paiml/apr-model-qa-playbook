[
  {
    "id": "00000000000000001893d5df499c830d",
    "gate_id": "G0-PULL-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Pull: acquire model via apr pull",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: model acquired via apr pull\n\u001b[1;36m=== APR Pull ===\u001b[0m\n\nModel: \u001b[36mmicrosoft\u001b[0m/\u001b[36mPhi-3.5-mini-instruct\u001b[0m (\u001b[33m2\u001b[0m shards)\n\n  \u001b[33mDownloading\u001b[0m model.safetensors.index.json\n  \u001b[33m↓\u001b[0m [1/2] model-00001-of-00002.safetensors... 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% \u001b[32mdone\u001b[0m\n  \u001b[33m↓\u001b[0m [2/2] model-00002-of-00002.safetensors... 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% \u001b[32mdone\u001b[0m\n  \u001b[32m✓\u001b[0m tokenizer.json\n  \u001b[32m✓\u001b[0m config.json\n  \u001b[32m✓\u001b[0m tokenizer_config.json\n  \u001b[32m✓\u001b[0m .apr-manifest.json (integrity checksums)\n\n\u001b[32m✓\u001b[0m Downloaded successfully\n  Path: \u001b[32m/home/noah/.apr/cache/hf/microsoft/Phi-3.5-mini-instruct/model.safetensors.index.json\u001b[0m\n  Shards: \u001b[33m2\u001b[0m\n\n\u001b[1;36mUsage:\u001b[0m\n  apr run /home/noah/.apr/cache/hf/microsoft/Phi-3.5-mini-instruct/model.safetensors.index.json\n  apr serve /home/noah/.apr/cache/hf/microsoft/Phi-3.5-mini-instruct/model.safetensors.index.json\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 87318
    },
    "timestamp": "2026-02-13T14:50:33.476155327Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6192aa082db",
    "gate_id": "G0-FORMAT-APR-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "G0 Format: prepare Apr workspace",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: converted to apr\n\u001b[1;36m=== Rosetta Stone Conversion ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\nTarget: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\n\n\u001b[33m--- Source Inspection ---\u001b[0m\n╭────────────┬───────────────╮\n│     Format │ SafeTensors   │\n├────────────┼───────────────┤\n│  File Size │ 7.12 GiB      │\n│    Tensors │ 195           │\n│ Parameters │ 3,821,079,552 │\n╰────────────┴───────────────╯\n\n\u001b[33mConverting...\u001b[0m\n\n\u001b[33m--- Target Inspection ---\u001b[0m\n╭──────────────┬───────────────╮\n│       Format │ APR           │\n├──────────────┼───────────────┤\n│    File Size │ 14.24 GiB     │\n│      Tensors │ 195           │\n│   Parameters │ 3,821,079,552 │\n│ Architecture │ phi3          │\n╰──────────────┴───────────────╯\n\n\u001b[1;36m=== Conversion Summary ===\u001b[0m\nPath: SafeTensors → APR\nDuration: 248577ms\nTensors: 195 -> 195\n\n\u001b[1;32mConversion successful\u001b[0m\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 248586
    },
    "timestamp": "2026-02-13T14:54:42.064487426Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d65d8fc5fd91",
    "gate_id": "G0-FORMAT-GGUF-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "G0 Format: prepare Gguf workspace",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: converted to gguf\n\u001b[1;36m=== Rosetta Stone Conversion ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\nTarget: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf\n\n\u001b[33m--- Source Inspection ---\u001b[0m\n╭────────────┬───────────────╮\n│     Format │ SafeTensors   │\n├────────────┼───────────────┤\n│  File Size │ 7.12 GiB      │\n│    Tensors │ 195           │\n│ Parameters │ 3,821,079,552 │\n╰────────────┴───────────────╯\n\n\u001b[33mConverting...\u001b[0m\n\n\u001b[33m--- Target Inspection ---\u001b[0m\n╭──────────────┬───────────────╮\n│       Format │ GGUF          │\n├──────────────┼───────────────┤\n│    File Size │ 2.32 GiB      │\n│      Tensors │ 259           │\n│   Parameters │ 3,821,079,552 │\n│ Architecture │ phi3          │\n│ Quantization │ 12            │\n╰──────────────┴───────────────╯\n\n\u001b[1;36m=== Conversion Summary ===\u001b[0m\nPath: SafeTensors → GGUF\nDuration: 293741ms\nTensors: 195 -> 259\n\n\u001b[33mWarning: Tensor count changed during conversion\u001b[0m\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 293754
    },
    "timestamp": "2026-02-13T14:59:35.819160457Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d661b82a4054",
    "gate_id": "G0-VALIDATE-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Validate: NaN/Inf/all-zeros tensor check",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: model-00001-of-00002.safetensors physics validated\nValidating output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model-00001-of-00002.safetensors...\n\n\n\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n  \u001b[1;36mValidate: SafeTensors (Rosetta Stone)\u001b[0m\n\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n╭─────────────────────────────────────────────────┬──────────────────────────┬──────────╮\n│ Tensor                                          │ Status                   │ Failures │\n├─────────────────────────────────────────────────┼──────────────────────────┼──────────┤\n│ model.embed_tokens.weight                       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.0.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.1.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.10.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.11.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.12.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.13.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.14.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.15.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.16.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.17.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.18.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.19.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.2.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.20.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.3.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.4.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.5.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.6.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.7.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.8.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.input_layernorm.weight           │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.mlp.down_proj.weight             │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.mlp.gate_up_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.post_attention_layernorm.weight  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.self_attn.o_proj.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.9.self_attn.qkv_proj.weight        │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n╰─────────────────────────────────────────────────┴──────────────────────────┴──────────╯\n\nVALID: 128 tensors checked, 0 contract violations (PMAT-235)\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 17857
    },
    "timestamp": "2026-02-13T14:59:53.676700960Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d663dcd76108",
    "gate_id": "G0-VALIDATE-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Validate: NaN/Inf/all-zeros tensor check",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: model-00002-of-00002.safetensors physics validated\nValidating output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model-00002-of-00002.safetensors...\n\n\n\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n  \u001b[1;36mValidate: SafeTensors (Rosetta Stone)\u001b[0m\n\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n╭─────────────────────────────────────────────────┬──────────────────────────┬──────────╮\n│ Tensor                                          │ Status                   │ Failures │\n├─────────────────────────────────────────────────┼──────────────────────────┼──────────┤\n│ lm_head.weight                                  │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.21.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.22.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.23.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.24.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.25.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.26.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.27.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.28.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.29.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.30.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.input_layernorm.weight          │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.mlp.down_proj.weight            │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.mlp.gate_up_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.post_attention_layernorm.weight │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.self_attn.o_proj.weight         │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.layers.31.self_attn.qkv_proj.weight       │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n│ model.norm.weight                               │ \u001b[1;32m✓\u001b[0m \u001b[1;32mPASS\u001b[0m │          │\n╰─────────────────────────────────────────────────┴──────────────────────────┴──────────╯\n\nVALID: 67 tensors checked, 0 contract violations (PMAT-235)\n",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 9205
    },
    "timestamp": "2026-02-13T15:00:02.881947236Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d663dcf7fa7c",
    "gate_id": "G0-INTEGRITY-CONFIG",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Integrity: config.json vs tensor metadata",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: config.json matches tensor metadata",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:00:02.884082831Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d663dd04dc74",
    "gate_id": "G0-LAYOUT-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Layout: tensor shape contract validation",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: Tensor layouts conform to contract\n  Rules checked: 132\n  Rules passed: 132",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:00:02.884927111Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d66427bbb2d8",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.3.self_attn.qkv_proj.weight\", \"model.layers.25.self_attn.o_proj.weight\", \"model.layers.30.post_attention_layernorm.weight\", \"model.layers.16.input_layernorm.weight\", \"model.layers.29.mlp.down_proj.weight\"], ...\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.3.mlp.down_proj.weight\", \"lm_head.weight\", \"model.layers.26.self_attn.o_proj.weight\", \"model.layers.17.post_attention_layernorm.weight\", \"model.layers.9.self_attn.qkv_proj.weight\"], ...\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\n",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.3.self_attn.qkv_proj.weight\", \"model.layers.25.self_attn.o_proj.weight\", \"model.layers.30.post_attention_layernorm.weight\", \"model.layers.16.input_layernorm.weight\", \"model.layers.29.mlp.down_proj.weight\"], ...\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.3.mlp.down_proj.weight\", \"lm_head.weight\", \"model.layers.26.self_attn.o_proj.weight\", \"model.layers.17.post_attention_layernorm.weight\", \"model.layers.9.self_attn.qkv_proj.weight\"], ...\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1253
    },
    "timestamp": "2026-02-13T15:00:04.138424352Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d696300c3139",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_apr_0000000000000001",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 1,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\n",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 214887
    },
    "timestamp": "2026-02-13T15:03:39.026288603Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6a414480c97",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_gguf_0000000000000002",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 2,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf\n",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf",
    "stderr": "error: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 59663
    },
    "timestamp": "2026-02-13T15:04:38.689985212Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6a46c092456",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_gpu_safetensors_0000000000000003",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 3,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.31.post_attention_layernorm.weight\", \"model.layers.20.self_attn.o_proj.weight\", \"model.layers.17.self_attn.o_proj.weight\", \"model.layers.17.mlp.down_proj.weight\", \"model.layers.5.input_layernorm.weight\"], ...\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.1.self_attn.o_proj.weight\", \"model.layers.5.post_attention_layernorm.weight\", \"model.layers.17.input_layernorm.weight\", \"model.layers.24.post_attention_layernorm.weight\", \"model.layers.26.self_attn.qkv_proj.weight\"], ...\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\n",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.31.post_attention_layernorm.weight\", \"model.layers.20.self_attn.o_proj.weight\", \"model.layers.17.self_attn.o_proj.weight\", \"model.layers.17.mlp.down_proj.weight\", \"model.layers.5.input_layernorm.weight\"], ...\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.1.self_attn.o_proj.weight\", \"model.layers.5.post_attention_layernorm.weight\", \"model.layers.17.input_layernorm.weight\", \"model.layers.24.post_attention_layernorm.weight\", \"model.layers.26.self_attn.qkv_proj.weight\"], ...\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 1472
    },
    "timestamp": "2026-02-13T15:04:40.162257290Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6c79405331b",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_gpu_apr_0000000000000004",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 4,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\n",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 32064 vocab, 61249 merges, 14 special tokens\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\nerror: Inference failed: Inference failed: Format error: [F-DATA-QUALITY-001] Tensor 'layers.0.ffn_up_weight': DENSITY FAILURE: 100.0% zeros (max 80%)\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 150994
    },
    "timestamp": "2026-02-13T15:07:11.156951030Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6d5abb01e9b",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_gpu_gguf_0000000000000005",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 5,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf\n",
    "output": "\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf",
    "stderr": "error: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE OUTPUT ---\n\u001b[36mInference tracing enabled (APR-TRACE-001)\u001b[0m\n  Trace level: basic\nerror: Inference failed: Inference failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found\n\n--- TRACE STDOUT ---\n\u001b[1;36m=== APR Run ===\u001b[0m\n\nSource: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 60526
    },
    "timestamp": "2026-02-13T15:08:11.683562276Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d6d5acc36f19",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_chat_cpu_safetensors_0000000000000006",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 6,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Chat Demo (Tiny Model) ===\u001b[0m\n\n\u001b[33mNote: Using tiny demo model. Pass .apr, .gguf, or .safetensors file for full model.\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m Demo format in 0.00s (0.0 MB)\n\u001b[32mDetected\u001b[0m \u001b[36mRaw\u001b[0m chat template\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m \n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 18
    },
    "timestamp": "2026-02-13T15:08:11.701605864Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7560227bdc3",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_chat_cpu_apr_0000000000000007",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 7,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (APR Format) ===\u001b[0m\n\n\u001b[36mUsing APR v2 format with mmap (Native Library Mandate)\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m APR format in 6.53s (15285.3 MB)\n\u001b[32mLoaded tokenizer from HuggingFace cache:\u001b[0m /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json (\u001b[2m151936 tokens\u001b[0m)\n\u001b[32mDetected\u001b[0m \u001b[36mPhi\u001b[0m chat template\n\u001b[92m[APR CUDA: NVIDIA GeForce RTX 4090 (24045 MB VRAM) — pre-cached]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday Thursday\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": "[AprV2ModelCuda] VRAM sufficient (23114 MB free), using full cache mode\n[AprV2ModelCuda] Pre-cached 8056 MB of weights on GPU (32 layers, 0 quantized, 160 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 375 MB\n[APR-LOAD] Embedding tensor 'model.embed_tokens.weight': dims=[32064, 3072], expected [vocab=32064, hidden=3072]\n[APR-LOAD] Embedding dims=[32064, 3072], using raw data (no transpose needed)\n[APR-LOAD] Token 0 embedding sample: [-0.0562, 0.0645, -0.0588, -0.0292, 0.0757]\n[APR-LOAD] Embedding loaded: 98500608 elements (vocab=32064 x hidden=3072)\n[APR-LOAD] LM head tensor 'lm_head.weight': dims=[32064, 3072], dtype=0, expected [vocab=32064, hidden=3072]\n[APR-LOAD] LM head loaded: 98500608 elements (hidden=3072 x vocab=32064)\n[APR-LOAD] LM head using F32 matmul (no Q4K/Q6K found)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 551188
    },
    "timestamp": "2026-02-13T15:17:22.890057761Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d75cfb018231",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_chat_cpu_gguf_0000000000000008",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 8,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (GGUF Format) ===\u001b[0m\n\n\u001b[36mUsing GGUF format with realizar inference engine\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m GGUF format in 1.21s (2490.1 MB)\n\u001b[32mLoaded\u001b[0m tokenizer with 32064 tokens\n\u001b[32mDetected\u001b[0m \u001b[36mPhi\u001b[0m chat template\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m [Error: Failed to create GGUF model: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found]\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": "[GGUF model parse failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found, will use CPU]\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 29944
    },
    "timestamp": "2026-02-13T15:17:52.834881892Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d75cfb61070e",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_chat_gpu_safetensors_0000000000000009",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 9,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Chat Demo (Tiny Model) ===\u001b[0m\n\n\u001b[33mNote: Using tiny demo model. Pass .apr, .gguf, or .safetensors file for full model.\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors.index.json\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m Demo format in 0.00s (0.0 MB)\n\u001b[32mDetected\u001b[0m \u001b[36mRaw\u001b[0m chat template\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m \n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 6
    },
    "timestamp": "2026-02-13T15:17:52.841141624Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d76f4cce2cb5",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_chat_gpu_apr_000000000000000a",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 10,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (APR Format) ===\u001b[0m\n\n\u001b[36mUsing APR v2 format with mmap (Native Library Mandate)\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m APR format in 7.28s (15285.3 MB)\n\u001b[32mLoaded tokenizer from HuggingFace cache:\u001b[0m /home/noah/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json (\u001b[2m151936 tokens\u001b[0m)\n\u001b[32mDetected\u001b[0m \u001b[36mPhi\u001b[0m chat template\n\u001b[92m[APR CUDA: NVIDIA GeForce RTX 4090 (24045 MB VRAM) — pre-cached]\u001b[0m\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m [Error: APR CUDA generate failed: Format error: No matching tensor found. Tried: [\"model.layers.0.mlp.gate_proj.weight\", \"layers.0.mlp.gate_proj.weight\", \"transformer.h.0.mlp.gate_proj.weight\", \"layers.0.feed_forward.w1.weight\", \"blk.0.ffn_gate.weight\"]]\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": "[AprV2ModelCuda] GH-201: Limited VRAM (11053 MB free), using layer streaming mode\n[AprV2ModelCuda] GH-201: Streaming mode - cached 375 MB (LM head ~375 MB, norms)\n[AprV2ModelCuda] GH-201: Layer weights will be streamed on-demand\n[AprV2ModelCuda] Cached embedding table: 375 MB\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 78675
    },
    "timestamp": "2026-02-13T15:19:11.516661062Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7766b9c4a41",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_chat_gpu_gguf_000000000000000b",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 11,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "\u001b[1;36m=== Model Chat (GGUF Format) ===\u001b[0m\n\n\u001b[36mUsing GGUF format with realizar inference engine\u001b[0m\n\n  \u001b[1;37mModel\u001b[0m: output/workspace/microsoft/Phi-3.5-mini-instruct/gguf/model.gguf\n  \u001b[1;37mChat Template\u001b[0m: Raw\n  \u001b[1;37mTemperature\u001b[0m: 0.7\n  \u001b[1;37mTop-P\u001b[0m: 0.9\n  \u001b[1;37mMax Tokens\u001b[0m: 512\n\n\u001b[1;37mCommands:\u001b[0m\n  /quit     Exit the chat\n  /clear    Clear conversation history\n  /system   Set system prompt\n  /help     Show help\n\n════════════════════════════════════════════════════════════\n\n\u001b[36mLoading model...\u001b[0m\n\u001b[32mLoaded\u001b[0m GGUF format in 1.37s (2490.1 MB)\n\u001b[32mLoaded\u001b[0m tokenizer with 32064 tokens\n\u001b[32mDetected\u001b[0m \u001b[36mPhi\u001b[0m chat template\n\u001b[1;32mYou: \u001b[0m\u001b[1;34mAssistant:\u001b[0m [Error: Failed to create GGUF model: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found]\n\n\u001b[1;32mYou: \u001b[0m\n\u001b[36mGoodbye!\u001b[0m",
    "stderr": "[GGUF model parse failed: Invalid shape: Tensor 'blk.0.ffn_up.weight' not found, will use CPU]\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 30581
    },
    "timestamp": "2026-02-13T15:19:42.098256654Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d776e4ec84f3",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_serve_cpu_safetensors_000000000000000c",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 12,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 2035
    },
    "timestamp": "2026-02-13T15:19:44.133557897Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d78380acbcce",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_serve_cpu_apr_000000000000000d",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 13,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 54152
    },
    "timestamp": "2026-02-13T15:20:38.286230239Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7a49cb2f500",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_serve_cpu_gguf_000000000000000e",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 14,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 52): ",
    "output": "",
    "stderr": "",
    "exit_code": 52,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 142204
    },
    "timestamp": "2026-02-13T15:23:00.490320546Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7a51496d03a",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_serve_gpu_safetensors_000000000000000f",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 15,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 2011
    },
    "timestamp": "2026-02-13T15:23:02.501742114Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7b3b35b649b",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_serve_gpu_apr_0000000000000010",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 16,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 62793
    },
    "timestamp": "2026-02-13T15:24:05.294967648Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7bf1f668de6",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_serve_gpu_gguf_0000000000000011",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 17,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 52): ",
    "output": "",
    "stderr": "",
    "exit_code": 52,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 49057
    },
    "timestamp": "2026-02-13T15:24:54.352279149Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7bf4a6f582b",
    "gate_id": "F-GOLDEN-RULE-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Golden Rule: convert → inference → diff",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Golden Rule: original inference failed: error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.layers.0.self_attn.q_proj.weight', 'blk.0.attn_q.weight', or 'layers.0.self_attn.q_proj.weight'. Available tensors (195 total): [\"model.layers.16.mlp.gate_up_proj.weight\", \"model.layers.22.mlp.gate_up_proj.weight\", \"model.layers.9.mlp.gate_up_proj.weight\", \"model.layers.5.input_layernorm.weight\", \"model.layers.1.post_attention_layernorm.weight\"], ...\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:24:55.074274155Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7c2f361d30c",
    "gate_id": "F-CONTRACT-I2-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Format contract invariant",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "I-2 Tensor Name Bijection: inspect failed: error: File not found: output/workspace/microsoft/Phi-3.5-mini-instruct/safetensors/model.safetensors\n",
    "output": "st: , apr: {\n  \"format\": \"APR\",\n  \"file_size\": 15285318916,\n  \"total_params\": 3821079552,\n  \"tensor_count\": 195,\n  \"architecture\": \"phi3\",\n  \"metadata_keys\": 3\n}\n",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:25:10.793639629Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001893d7cd57435463",
    "gate_id": "F-CONTRACT-I3-001",
    "scenario": {
      "id": "Phi-3.5-mini-instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "microsoft",
        "name": "Phi-3.5-mini-instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Format contract invariant",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "I-3 No Silent Fallbacks: check failed: error: Validation failed: Model self-test failed\n",
    "output": "\n\u001b[1;36m=== Model Self-Test (PMAT-112: Real Validation) ===\u001b[0m\nModel: \u001b[36moutput/workspace/microsoft/Phi-3.5-mini-instruct/apr/model.apr\u001b[0m\n\n┌─────┬─────────────────────┬──────────────────────────────────────┬──────┐\n│  #  │      Component      │               Details                │ Pass │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 1   │ Tokenizer           │ tokens=[1, 2]                        │ \u001b[31m❌   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 2   │ Embedding           │ Found embedding tensor               │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 3   │ Positional Encoding │ RoPE computed inline                 │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 4   │ Q/K/V Projection    │ Missing Q/K/V                        │ \u001b[31m❌   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 5   │ Attention Scores    │ Attention output found               │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 6   │ Feed-Forward (MLP)  │ Missing MLP                          │ \u001b[31m❌   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 7   │ Layer Norm          │ 32 layers                            │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 8   │ LM Head             │ vocab_size=32064                     │ \u001b[32m✅   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 9   │ Logits → Probs      │ Forward failed: Format error: No ... │ \u001b[31m❌   \u001b[0m │\n├─────┼─────────────────────┼──────────────────────────────────────┼──────┤\n│ 10  │ Sampler/Decode      │ Forward failed: Format error: No ... │ \u001b[31m❌   \u001b[0m │\n└─────┴─────────────────────┴──────────────────────────────────────┴──────┘\n\n\u001b[1;31m❌ 5/10 STAGES PASSED. CHECK STAGE LOGS.\u001b[0m\n",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-13T15:25:55.419036075Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  }
]