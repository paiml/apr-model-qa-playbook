[
  {
    "id": "00000000000000001890b46efa371cd8",
    "gate_id": "G0-INTEGRITY-CONFIG",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "G0 Integrity: config.json vs tensor metadata",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "G0 PASS: config.json matches tensor metadata",
    "stderr": null,
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:04:02.293924624Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b47080f3d82d",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.11.attn_k.weight\", \"blk.12.attn_q.bias\", \"blk.1.attn_k.weight\", \"blk.1.attn_norm.weight\", \"blk.1.attn_output.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.10.attn_v.bias\", \"blk.11.attn_output.weight\", \"blk.11.ffn_norm.weight\", \"blk.12.attn_k.bias\", \"blk.1.attn_k.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.11.attn_k.weight\", \"blk.12.attn_q.bias\", \"blk.1.attn_k.weight\", \"blk.1.attn_norm.weight\", \"blk.1.attn_output.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.10.attn_v.bias\", \"blk.11.attn_output.weight\", \"blk.11.ffn_norm.weight\", \"blk.12.attn_k.bias\", \"blk.1.attn_k.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 6555
    },
    "timestamp": "2026-02-03T10:04:08.849410523Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4875270ec23",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_apr_0000000000000001",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 1,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 98003
    },
    "timestamp": "2026-02-03T10:05:46.853327081Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b48b933343bb",
    "gate_id": "F-A1-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_gguf_0000000000000002",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 2,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf\n\ntokens: 15\nlatency: 18023.41ms\nmodel: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 6655.0ms (4.8 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 0.8,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 18266
    },
    "timestamp": "2026-02-03T10:06:05.119673917Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b48d4de2bf20",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_safetensors_0000000000000003",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 3,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.11.attn_v.weight\", \"blk.10.ffn_up.weight\", \"blk.0.attn_v.weight\", \"blk.0.attn_v.bias\", \"blk.12.attn_q.bias\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.1.attn_q.bias\", \"blk.0.ffn_up.weight\", \"blk.10.attn_output.weight\", \"blk.10.attn_v.bias\", \"blk.11.ffn_gate.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.11.attn_v.weight\", \"blk.10.ffn_up.weight\", \"blk.0.attn_v.weight\", \"blk.0.attn_v.bias\", \"blk.12.attn_q.bias\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.1.attn_q.bias\", \"blk.0.ffn_up.weight\", \"blk.10.attn_output.weight\", \"blk.10.attn_v.bias\", \"blk.11.ffn_gate.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 7427
    },
    "timestamp": "2026-02-03T10:06:12.546703567Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4a3dcf1eed0",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_apr_0000000000000004",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 4,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22643 MB of weights on GPU (28 layers, 180 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 96889
    },
    "timestamp": "2026-02-03T10:07:49.436121084Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4a7c2087ac5",
    "gate_id": "F-A2-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_gguf_0000000000000005",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 5,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf\n\ntokens: 21\nlatency: 16600.45ms\nmodel: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 8931.2ms (3.6 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.3,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 16728
    },
    "timestamp": "2026-02-03T10:08:06.164482998Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4a9267bfc5f",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_chat_cpu_safetensors_0000000000000006",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 6,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.attn_norm.weight\", \"blk.11.attn_norm.weight\", \"blk.11.attn_v.bias\", \"blk.11.ffn_up.weight\", \"blk.10.attn_v.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.1.attn_norm.weight\", \"blk.1.ffn_norm.weight\", \"blk.11.ffn_up.weight\", \"blk.12.attn_q.weight\", \"blk.0.attn_v.bias\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.attn_norm.weight\", \"blk.11.attn_norm.weight\", \"blk.11.attn_v.bias\", \"blk.11.ffn_up.weight\", \"blk.10.attn_v.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.1.attn_norm.weight\", \"blk.1.ffn_norm.weight\", \"blk.11.ffn_up.weight\", \"blk.12.attn_q.weight\", \"blk.0.attn_v.bias\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 5980
    },
    "timestamp": "2026-02-03T10:08:12.144741982Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4bf9a18a774",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_chat_cpu_apr_0000000000000007",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 7,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22801 MB of weights on GPU (28 layers, 178 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22801 MB of weights on GPU (28 layers, 178 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22801 MB of weights on GPU (28 layers, 178 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22801 MB of weights on GPU (28 layers, 178 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 96428
    },
    "timestamp": "2026-02-03T10:09:48.573669407Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4c3551761d3",
    "gate_id": "F-A3-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_chat_cpu_gguf_0000000000000008",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 8,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf\n\ntokens: 19\nlatency: 15795.27ms\nmodel: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 8307.5ms (3.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.2,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 16022
    },
    "timestamp": "2026-02-03T10:10:04.595828106Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4c4bf064518",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_chat_gpu_safetensors_0000000000000009",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 9,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.ffn_norm.weight\", \"blk.11.attn_v.bias\", \"blk.1.attn_v.weight\", \"blk.0.attn_norm.weight\", \"blk.10.attn_norm.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.1.attn_v.weight\", \"blk.10.attn_norm.weight\", \"blk.1.attn_norm.weight\", \"blk.0.attn_k.bias\", \"blk.0.ffn_down.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.ffn_norm.weight\", \"blk.11.attn_v.bias\", \"blk.1.attn_v.weight\", \"blk.0.attn_norm.weight\", \"blk.10.attn_norm.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.1.attn_v.weight\", \"blk.10.attn_norm.weight\", \"blk.1.attn_norm.weight\", \"blk.0.attn_k.bias\", \"blk.0.ffn_down.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 6072
    },
    "timestamp": "2026-02-03T10:10:10.668058461Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4dc95172f8e",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_chat_gpu_apr_000000000000000a",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 10,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22801 MB of weights on GPU (28 layers, 178 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU GEMM cached' not supported: CUDA GEMM with cached weight 'blk.24.ffn_up.weight' failed: Invalid launch config: Weight 'blk.24.ffn_up.weight' not cached\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22861 MB of weights on GPU (28 layers, 184 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22801 MB of weights on GPU (28 layers, 178 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU GEMM cached' not supported: CUDA GEMM with cached weight 'blk.24.ffn_up.weight' failed: Invalid launch config: Weight 'blk.24.ffn_up.weight' not cached\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22861 MB of weights on GPU (28 layers, 184 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 102375
    },
    "timestamp": "2026-02-03T10:11:53.043738930Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4dfb4628727",
    "gate_id": "F-A4-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_chat_gpu_gguf_000000000000000b",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "chat",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 11,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf\n\ntokens: 14\nlatency: 13111.99ms\nmodel: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 5505.6ms (5.8 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.1,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 13409
    },
    "timestamp": "2026-02-03T10:12:06.453671739Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4e12b8d19d9",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_serve_cpu_safetensors_000000000000000c",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 12,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.12.attn_q.bias\", \"blk.11.attn_norm.weight\", \"blk.0.attn_k.bias\", \"blk.10.ffn_down.weight\", \"blk.0.attn_v.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.10.attn_v.weight\", \"blk.0.attn_k.bias\", \"blk.12.ffn_up.weight\", \"blk.10.attn_q.bias\", \"blk.1.attn_q.bias\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.12.attn_q.bias\", \"blk.11.attn_norm.weight\", \"blk.0.attn_k.bias\", \"blk.10.ffn_down.weight\", \"blk.0.attn_v.weight\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.10.attn_v.weight\", \"blk.0.attn_k.bias\", \"blk.12.ffn_up.weight\", \"blk.10.attn_q.bias\", \"blk.1.attn_q.bias\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 6294
    },
    "timestamp": "2026-02-03T10:12:12.747918146Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4f81fcfddb2",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_serve_cpu_apr_000000000000000d",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 13,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22661 MB of weights on GPU (28 layers, 185 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU GEMM' not supported: CUDA GEMM failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.attn_output.weight' not cached\n[AprV2ModelCuda] Pre-cached 22682 MB of weights on GPU (28 layers, 181 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22661 MB of weights on GPU (28 layers, 185 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU GEMM' not supported: CUDA GEMM failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.25.attn_output.weight' not cached\n[AprV2ModelCuda] Pre-cached 22682 MB of weights on GPU (28 layers, 181 quantized, 95 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 98587
    },
    "timestamp": "2026-02-03T10:13:51.335214765Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4fb249c676f",
    "gate_id": "F-A5-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_serve_cpu_gguf_000000000000000e",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 14,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf\n\ntokens: 16\nlatency: 12753.79ms\nmodel: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 5454.9ms (5.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.3,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 12965
    },
    "timestamp": "2026-02-03T10:14:04.300630402Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b4fc98b64fa7",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_serve_gpu_safetensors_000000000000000f",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 15,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.attn_norm.weight\", \"blk.11.attn_norm.weight\", \"blk.11.ffn_gate.weight\", \"blk.0.ffn_norm.weight\", \"blk.10.attn_q.bias\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.attn_output.weight\", \"blk.1.attn_v.bias\", \"blk.11.attn_q.weight\", \"blk.1.attn_q.weight\", \"blk.0.attn_norm.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors",
    "stderr": "error: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.attn_norm.weight\", \"blk.11.attn_norm.weight\", \"blk.11.ffn_gate.weight\", \"blk.0.ffn_norm.weight\", \"blk.10.attn_q.bias\"], ...\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\nerror: Inference failed: Inference failed: Operation 'get_tensor_auto' not supported: Tensor not found with names: 'model.embed_tokens.weight', 'token_embd.weight', or 'embed_tokens.weight'. Available tensors (64 total): [\"blk.0.attn_output.weight\", \"blk.1.attn_v.bias\", \"blk.11.attn_q.weight\", \"blk.1.attn_q.weight\", \"blk.0.attn_norm.weight\"], ...\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/safetensors/model.safetensors\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 6242
    },
    "timestamp": "2026-02-03T10:14:10.543453903Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b5139265d8fa",
    "gate_id": "G2-BASIC",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_serve_gpu_apr_0000000000000010",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "apr",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 16,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Falsified",
    "reason": "Command failed (exit 8): [PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22529 MB of weights on GPU (28 layers, 182 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22557 MB of weights on GPU (28 layers, 181 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU GEMM cached' not supported: CUDA GEMM with cached weight 'blk.24.ffn_up.weight' failed: Invalid launch config: Weight 'blk.24.ffn_up.weight' not cached\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr",
    "stderr": "[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_gate.weight' not cached\n[AprV2ModelCuda] Pre-cached 22529 MB of weights on GPU (28 layers, 182 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU Q4K batched GEMV cached' not supported: CUDA batched Q4K GEMV 'blk.0.ffn_gate.weight' failed: GPU memory allocation failed: CUDA driver error: CUDA_ERROR_OUT_OF_MEMORY (code: 2)\n\n--- TRACE OUTPUT ---\nInference tracing enabled (APR-TRACE-001)\n  Trace level: basic\n[PMAT-171] Loaded embedded BPE tokenizer: 152064 vocab, 151387 merges, 23 special tokens\n[AprV2ModelCuda] Warning: Could not build indexed weights: Invalid launch config: PAR-043: Quantized weight 'blk.24.ffn_up.weight' not cached\n[AprV2ModelCuda] Pre-cached 22557 MB of weights on GPU (28 layers, 181 quantized, 94 F32 tensors)\n[AprV2ModelCuda] Cached embedding table: 2079 MB\nerror: Inference failed: Inference failed: Inference error: GPU generation failed: Operation 'GPU GEMM cached' not supported: CUDA GEMM with cached weight 'blk.24.ffn_up.weight' failed: Invalid launch config: Weight 'blk.24.ffn_up.weight' not cached\n\n--- TRACE STDOUT ---\n=== APR Run ===\n\nSource: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/apr/model.apr\n",
    "exit_code": 8,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 98678
    },
    "timestamp": "2026-02-03T10:15:49.221763752Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516a92298cd",
    "gate_id": "F-A6-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_serve_gpu_gguf_0000000000000011",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "serve",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "What is 2+2?",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 17,
      "trace_level": "none",
      "oracle_type": "arithmetic"
    },
    "outcome": "Corroborated",
    "reason": "Test passed",
    "output": "Source: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf\n\ntokens: 18\nlatency: 13028.36ms\nmodel: /home/noah/.cache/apr-models/qwen2-5-coder-7b-instruct/gguf/model.gguf",
    "stderr": "Generated 32 tokens in 5429.0ms (5.9 tok/s)\n",
    "exit_code": 0,
    "metrics": {
      "tokens_per_second": 1.4,
      "time_to_first_token_ms": null,
      "total_tokens": 32,
      "memory_peak_mb": null,
      "duration_ms": 13266
    },
    "timestamp": "2026-02-03T10:16:02.488133417Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516a9727ad9",
    "gate_id": "F-CONV-G-A",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Convert Gguf to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.493369009Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516a9955c24",
    "gate_id": "F-CONV-G-A",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Convert Gguf to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.495652435Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516a9c4078e",
    "gate_id": "F-CONV-A-G",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Convert Apr to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.498712976Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516a9f195ee",
    "gate_id": "F-CONV-A-G",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Convert Apr to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.501698744Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516aa1989e9",
    "gate_id": "F-CONV-G-S",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Convert Gguf to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.504314452Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516aa3f663b",
    "gate_id": "F-CONV-G-S",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Convert Gguf to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.506795096Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516aa68b429",
    "gate_id": "F-CONV-S-G",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Convert SafeTensors to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.509503024Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516aa90796e",
    "gate_id": "F-CONV-S-G",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Convert SafeTensors to Gguf",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.512109198Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516aab8d82b",
    "gate_id": "F-CONV-A-S",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Convert Apr to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.514754901Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516aadaafb5",
    "gate_id": "F-CONV-A-S",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Convert Apr to SafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.516972096Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ab002f32",
    "gate_id": "F-CONV-S-A",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Convert SafeTensors to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.519430026Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ab3105fb",
    "gate_id": "F-CONV-S-A",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Convert SafeTensors to Apr",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Conversion infrastructure error: Execution error: Inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.522633463Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ab8641a4",
    "gate_id": "F-CONV-RT-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "gguf",
      "prompt": "Round-trip conversion",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Round-trip failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.528216722Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516abd7b514",
    "gate_id": "F-CONV-RT-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_gguf_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "gguf",
      "prompt": "Round-trip conversion",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Round-trip failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.533556522Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ac25f16f",
    "gate_id": "F-CONV-RT-002",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Multi-hop: SafeTensorsAprGgufSafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Multi-hop chain failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.538682139Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ac72bb1c",
    "gate_id": "F-CONV-RT-002",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Multi-hop: SafeTensorsAprGgufSafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Multi-hop chain failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.543714384Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516acb9751a",
    "gate_id": "F-CONV-RT-003",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "safetensors",
      "prompt": "Multi-hop: SafeTensorsAprGgufAprSafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Multi-hop chain failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.548349548Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516acfc4218",
    "gate_id": "F-CONV-RT-003",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_safetensors_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "safetensors",
      "prompt": "Multi-hop: SafeTensorsAprGgufAprSafeTensors",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Multi-hop chain failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.552726952Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ad2a7e22",
    "gate_id": "F-CONV-IDEM-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Idempotency: GGUFAPR twice",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Idempotency test failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.555759661Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ad50173e",
    "gate_id": "F-CONV-IDEM-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Idempotency: GGUFAPR twice",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Idempotency test failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.558220655Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ad72f229",
    "gate_id": "F-CONV-COM-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Commutativity: GGUFAPR vs GGUFSTAPR",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Commutativity test failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.560505402Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516ad999b7b",
    "gate_id": "F-CONV-COM-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_gpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "gpu",
      "format": "apr",
      "prompt": "Commutativity: GGUFAPR vs GGUFSTAPR",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Commutativity test failed: Execution error: Conversion failed: error: Validation failed: Source inspection failed: Invalid model format: No file extension found\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.563039087Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  },
  {
    "id": "00000000000000001890b516adbb9a24",
    "gate_id": "F-GOLDEN-RULE-001",
    "scenario": {
      "id": "Qwen2.5-Coder-7B-Instruct_run_cpu_apr_0000000000000000",
      "model": {
        "org": "Qwen",
        "name": "Qwen2.5-Coder-7B-Instruct",
        "variant": null
      },
      "modality": "run",
      "backend": "cpu",
      "format": "apr",
      "prompt": "Golden Rule: convert  inference  diff",
      "temperature": 0.0,
      "max_tokens": 32,
      "seed": 0,
      "trace_level": "none",
      "oracle_type": "garbage"
    },
    "outcome": "Falsified",
    "reason": "Golden Rule: original inference failed: error: Inference failed: Inference failed: Security error: Invalid model file extension: '.'. Expected one of: gguf, safetensors, apr, bin\n",
    "output": "N/A",
    "stderr": null,
    "exit_code": null,
    "metrics": {
      "tokens_per_second": null,
      "time_to_first_token_ms": null,
      "total_tokens": null,
      "memory_peak_mb": null,
      "duration_ms": 0
    },
    "timestamp": "2026-02-03T10:16:02.565266627Z",
    "host": {
      "hostname": "noah-Lambda-Vector",
      "os": "linux",
      "cpu": "unknown",
      "gpu": null,
      "apr_version": "unknown"
    },
    "metadata": {}
  }
]