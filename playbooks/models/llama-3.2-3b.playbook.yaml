# Llama 3.2 3B Model Qualification Playbook
# Model: meta-llama/Llama-3.2-3B-Instruct
# Generated: 2026-01-30
# MQS Target: 85+

name: llama-3.2-3b
version: "1.0.0"
description: "Qualification playbook for Llama-3.2-3B-Instruct"

model:
  hf_repo: "meta-llama/Llama-3.2-3B-Instruct"
  formats:
    - gguf
    - safetensors
    - apr
  quantizations:
    - q4_k_m
    - q5_k_m
    - q8_0
  size_category: small

test_matrix:
  modalities:
    - run
    - chat
    - serve
  backends:
    - cpu
    - gpu
  scenario_count: 100
  seed: 42
  timeout_ms: 90000

gates:
  g1_model_loads: "true"
  g2_basic_inference: "true"
  g3_no_crashes: "true"
  g4_output_quality: "true"

oracles:
  - type: arithmetic
    config:
      tolerance: 0.01
  - type: garbage
    config:
      max_repetition_ratio: 0.3
      min_unique_chars: 10
  - type: code_syntax
    config:
      languages:
        - python
        - rust
        - javascript
  - type: response
    config:
      min_relevance: 0.3

failure_policy: stop_on_p0

property_tests:
  - name: arithmetic_correctness
    generator: "arithmetic(a: i32, b: i32, op: +|-|*)"
    oracle: "eval(a op b) == parse_int(output)"
    count: 50
  - name: general_qa
    generator: "qa_prompt()"
    oracle: "is_coherent(output)"
    count: 50

# Differential Testing (v1.3.0)
differential_tests:
  tensor_diff:
    enabled: "true"
    filter: "embed,lm_head"
    gates: ["F-ROSETTA-DIFF-001", "F-ROSETTA-DIFF-002"]
  inference_compare:
    enabled: "true"
    prompt: "What is 2+2?"
    max_tokens: 10
    tolerance: 1e-5
    gates: ["F-ROSETTA-INF-001", "F-ROSETTA-INF-003"]
  fingerprint:
    enabled: "true"
    tensors: "all"
    stats: ["mean", "std", "min", "max", "checksum"]
    gates: ["F-ROSETTA-FP-001", "F-ROSETTA-FP-002"]
  validate_stats:
    enabled: "true"
    reference: "llama-3.2-3b-reference.json"
    tolerance:
      layernorm: 0.001
      embedding: 0.1
      attention: 0.01
    gates: ["F-ROSETTA-STATS-001", "F-ROSETTA-STATS-002", "F-ROSETTA-STATS-005", "F-ROSETTA-STATS-006"]

# Profile CI Assertions (v1.3.0)
profile_ci:
  enabled: "true"
  warmup: 3
  measure: 10
  assertions:
    min_throughput: 5.0
    max_p99_ms: 800
  gates: ["F-PROFILE-CI-001", "F-PROFILE-CI-002"]

# Trace Payload (v1.3.0)
trace_payload:
  enabled: "true"
  prompt: "What is machine learning?"
  gates: ["F-TRACE-PAYLOAD-001", "F-TRACE-PAYLOAD-002"]

metadata:
  author: "apr-qa-gen"
  architecture: "llama"
  expected_mqs: 87
  tags:
    - general
    - instruct
    - small
