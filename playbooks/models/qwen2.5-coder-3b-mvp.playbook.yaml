# Qwen2.5-Coder-3B MVP Playbook
# Kernel profile: GQA + RMSNorm + SiLU + SwiGLU + RoPE + BiasAdd
# Full surface coverage: 3 formats × 2 backends × 3 modalities = 18 tests
name: qwen2.5-coder-3b-mvp
version: "2.0.0"
description: "MVP certification for Qwen2.5-Coder-3B-Instruct (kernel profile-driven)"

model:
  hf_repo: "Qwen/Qwen2.5-Coder-3B-Instruct"
  formats:
    - safetensors  # Ground truth (HuggingFace source)
    - apr          # APR native format
    - gguf         # Third-party format
  quantizations:
    - q4_k_m
  size_category: small
  expected_hidden_dim: 2048
  expected_num_layers: 36
  expected_num_heads: 16
  expected_num_kv_heads: 2
  expected_vocab_size: 151936
  expected_intermediate_dim: 11008
  family: qwen2
  size_variant: "3b"

test_matrix:
  modalities:
    - run
    - chat
    - serve
  backends:
    - cpu
    - gpu
  scenario_count: 1
  seed: 42
  timeout_ms: 120000
  prompts:
    # GQA multi-turn (stress KV cache sharing across head groups)
    - "Given x=5 and y=3, what is x*y? Then what is the result plus 10?"
    - "List the first 5 prime numbers. Now sum them."
    - "Define a function add(a,b) that returns a+b. What does add(3,4) return?"
    # RoPE long-context (position encoding extrapolation)
    - "Write a detailed step-by-step solution to: What is 123 * 456? Show all intermediate multiplication steps."
    - "List the numbers from 1 to 20, then sum them all. What is the final sum?"
    # Bias precision (floating-point accumulation)
    - "What is 0.1 + 0.2? Give a precise answer."
    - "Calculate 999 + 1."
    - "What is 1000000 - 999999?"
    # Arithmetic verification (deterministic correctness)
    - "What is 2+2?"
    - "Calculate 7*8"
    - "What is 15-7?"
    - "What is 100/4?"
    # Code completion (full token generation pipeline)
    - "def fibonacci(n):"
    - "fn main() {"
    - "Write a Python function that checks if a number is prime."

kernel_profile:
  family: qwen2
  kernel_ops:
    - FusedQ4kMatvec
    - FusedQ6kMatvec
    - GroupedQueryAttention
    - RmsNorm
    - Silu
    - SwiGlu
    - Rope
    - BiasAdd
  prompt_count: 15
  long_context: true

gates:
  g1_model_loads: "true"
  g2_basic_inference: "true"
  g3_no_crashes: "true"
  g4_output_quality: "true"

oracles:
  - type: arithmetic
    config:
      tolerance: 0.01
  - type: garbage
    config:
      max_repetition_ratio: 0.3
      min_unique_chars: 10

failure_policy: collect_all

# 6-column profiling: 3 formats × 2 backends
profile_ci:
  enabled: "true"
  warmup: 1
  measure: 2
  formats: [safetensors, apr, gguf]  # safetensors = ground truth
  backends: [cpu, gpu]
  assertions:
    min_throughput_cpu: 5.0
    min_throughput_gpu: 50.0

# Format contract invariant tests (GH-190/191 Five-Whys)
contract_tests:
  invariants: ["I-2", "I-3"]  # I-4/I-5 removed: BF16→Q4K cross-quantization comparison invalid

metadata:
  author: "apr-qa-gen"
  tier: "mvp"
  architecture: "qwen2"
  tags:
    - code
    - instruct
    - mvp
