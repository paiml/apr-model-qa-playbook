# Llama-3.3-70B-Instruct MVP Playbook
# Full surface coverage: 3 formats × 2 backends × 3 modalities = 18 tests
name: llama-3.3-70b-mvp
version: "1.0.0"
description: "MVP certification for meta-llama/Llama-3.3-70B-Instruct"

model:
  hf_repo: "meta-llama/Llama-3.3-70B-Instruct"
  formats:
    - safetensors  # Ground truth (HuggingFace source)
    - apr          # APR native format
    - gguf         # Third-party format
  quantizations:
    - q4_k_m
  size_category: huge

test_matrix:
  modalities:
    - run
    - chat
    - serve
  backends:
    - cpu
  scenario_count: 1
  seed: 42
  timeout_ms: 600000

gates:
  g1_model_loads: "true"
  g2_basic_inference: "true"
  g3_no_crashes: "true"
  g4_output_quality: "true"

oracles:
  - type: arithmetic
    config:
      tolerance: 0.01
  - type: garbage
    config:
      max_repetition_ratio: 0.3
      min_unique_chars: 10

failure_policy: collect_all

profile_ci:
  enabled: "true"
  warmup: 1
  measure: 2
  formats: [safetensors, apr, gguf]
  backends: [cpu]
  assertions:
    min_throughput_cpu: 1.0
    min_throughput_gpu: 0.0

contract_tests:
  invariants: ["I-2", "I-3"]

metadata:
  author: "apr-qa-gen"
  tier: "mvp"
  architecture: "llama"
  tags:
    - instruct
    - mvp
    - cpu-only
